{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Mount google drive"],"metadata":{"id":"82hmvOXZdHWO"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fl0pF_BwScB9","executionInfo":{"status":"ok","timestamp":1763057933398,"user_tz":-60,"elapsed":16364,"user":{"displayName":"Alessio Zattoni","userId":"03546066515652404413"}},"outputId":"5556ae2e-f7da-4443-b7b2-ce9b8daaa9dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#Download the dataset"],"metadata":{"id":"6YuqwIcsdLJY"}},{"cell_type":"code","source":["import os\n","import urllib.request\n","\n","url = \"https://nrvis.com/download/data/asn/mammalia-dolphin-florida-overall.zip\"\n","dataset_zip = \"mammalia-dolphin-florida-overall.zip\"\n","filename = \"mammalia-dolphin-florida-overall.edges\"\n","output_dir = \"/content/drive/MyDrive/GraphLA/clustering\"\n","data_dir = \"/content/drive/MyDrive/GraphLA/data\"\n","\n","os.makedirs(output_dir, exist_ok=True)\n","os.makedirs(data_dir, exist_ok=True)\n","\n","if not os.path.exists(dataset_zip):\n","    print(f\"Downloading {dataset_zip}...\")\n","    try:\n","        urllib.request.urlretrieve(url, dataset_zip)\n","        print(\"Download completed!\")\n","    except Exception as e:\n","        print(f\"Error during download: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omcuH_T1PO5u","outputId":"0147f411-86d9-499a-b2b8-56d23f511ad8","executionInfo":{"status":"ok","timestamp":1763057934033,"user_tz":-60,"elapsed":618,"user":{"displayName":"Alessio Zattoni","userId":"03546066515652404413"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading mammalia-dolphin-florida-overall.zip...\n","Download completed!\n"]}]},{"cell_type":"markdown","source":["#Unzip the dataset"],"metadata":{"id":"k-PTPwZ8dNoL"}},{"cell_type":"code","source":["import zipfile\n","\n","with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n","  zip_ref.extractall('.')"],"metadata":{"id":"y6TfxYoNRVqD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Define some utility method"],"metadata":{"id":"qrbOoeuodPGf"}},{"cell_type":"code","source":["import pickle\n","import random\n","import sys\n","import os\n","from os import path\n","\n","import networkx as nx\n","import torch\n","\n","def load_graph(file_path: str) -> nx.Graph:\n","    if not path.exists(file_path):\n","        raise FileNotFoundError(f\"File not found: {file_path}\")\n","\n","    G = nx.Graph()\n","    with open(file_path, \"r\") as f:\n","        for line in f:\n","            parts = line.strip().split()\n","            if len(parts) < 2:\n","                continue\n","            u, v = parts[0], parts[1]\n","            w = float(parts[2])\n","            G.add_edge(u, v, weight=w)\n","\n","    return G\n","\n","\n","def load_cluster_info(clusters_path: str = os.path.join(data_dir, 'cluster_info.pkl')):\n","    if not os.path.exists(clusters_path):\n","        print(f\"File '{clusters_path}' not found. Please run 'clustering.ipynb' first to generate it.\")\n","        raise FileNotFoundError()\n","\n","    with open(clusters_path, \"rb\") as f:\n","        return pickle.load(f)"],"metadata":{"id":"jVSgZbazPVrH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Main"],"metadata":{"id":"ePykEnzMdjDs"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfSR65CnOs2N","outputId":"b6e9ca89-2ca6-42db-c8f0-73eac76b0558","executionInfo":{"status":"ok","timestamp":1763057976329,"user_tz":-60,"elapsed":45,"user":{"displayName":"Alessio Zattoni","userId":"03546066515652404413"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving training set...\n","Training set saved successfully\n"]}],"source":["import pickle\n","\n","# 1) Load the graph\n","G = load_graph(filename)\n","\n","# 2) Load cluster information (see src/script/clustering.py)\n","cluster_info = load_cluster_info()\n","\n","# 3) Create the training set\n","node_list = list(G.nodes)\n","node_id_to_idx = {n: i for i, n in enumerate(node_list)}\n","\n","followers = [f for c in cluster_info for f in c[\"followers\"]]\n","leaders = [c[\"leader\"] for c in cluster_info]\n","\n","pos_edges = [\n","    (node_id_to_idx[f], node_id_to_idx[l])\n","    for l in leaders\n","    for f in followers\n","    if G.has_edge(f, l)\n","]\n","\n","all_pairs = [(node_id_to_idx[f], node_id_to_idx[l])\n","             for l in leaders\n","             for f in followers\n","             if f != l]\n","\n","possible_neg_edges = list(set(all_pairs) - set(pos_edges))\n","\n","# 4) Save the training set\n","print(\"Saving training set...\")\n","\n","try:\n","    with open(os.path.join(data_dir, 'positive_edges.pkl') ,\"wb\") as f:\n","        pickle.dump(pos_edges, f)\n","\n","    with open(os.path.join(data_dir, 'negative_edges.pkl'), \"wb\") as f:\n","        pickle.dump(possible_neg_edges, f)\n","\n","except (OSError, IOError) as e:\n","    print(f\"Some error occur: {e}\")\n","else:\n","    print(\"Training set saved successfully\")"]}]}